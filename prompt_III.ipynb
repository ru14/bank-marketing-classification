{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bank Marketing Classification Analysis\n",
    "\n",
    "This notebook implements a complete machine learning pipeline for predicting term deposit subscriptions based on bank marketing campaign data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "from time import time\n",
    "\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier, plot_tree\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, classification_report, confusion_matrix,\n",
    "    precision_score, recall_score, f1_score, roc_auc_score, roc_curve\n",
    ")\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "sns.set_style('whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (10, 6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 1: Understanding the Data\n",
    "\n",
    "The dataset represents **17 marketing campaigns** conducted between **May 2008 and November 2010** by a Portuguese banking institution. The campaigns involved phone calls to clients to promote term deposit subscriptions.\n",
    "\n",
    "This analysis follows the **CRISP-DM (Cross-Industry Standard Process for Data Mining)** methodology, which provides a structured approach to planning and executing data mining projects. The CRISP-DM framework includes six phases:\n",
    "1. Business Understanding\n",
    "2. Data Understanding\n",
    "3. Data Preparation\n",
    "4. Modeling\n",
    "5. Evaluation\n",
    "6. Deployment\n",
    "\n",
    "**Reference:** Shearer, C. (2000). The CRISP-DM model: the new blueprint for data mining. *Journal of Data Warehousing*, 5(4), 13-22."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 2: Read in the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the dataset\n",
    "df = pd.read_csv('data/bank-additional/bank-additional-full.csv', sep=';')\n",
    "\n",
    "# Display the shape of the dataset\n",
    "print(f\"Dataset shape: {df.shape}\")\n",
    "print(f\"Number of samples: {df.shape[0]}\")\n",
    "print(f\"Number of features: {df.shape[1]}\")\n",
    "print()\n",
    "\n",
    "# Display the first few rows\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 3: Understanding the Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for missing values\n",
    "print(\"Missing values per column:\")\n",
    "print(df.isnull().sum())\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check data types\n",
    "print(\"Data types:\")\n",
    "print(df.dtypes)\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for 'unknown' values in categorical columns\n",
    "print(\"Count of 'unknown' values in categorical columns:\")\n",
    "categorical_cols = df.select_dtypes(include=['object']).columns\n",
    "for col in categorical_cols:\n",
    "    unknown_count = (df[col] == 'unknown').sum()\n",
    "    if unknown_count > 0:\n",
    "        print(f\"{col}: {unknown_count} ({unknown_count/len(df)*100:.2f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get basic statistics\n",
    "print(\"Descriptive statistics for numerical features:\")\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display information about the dataset\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Notes on Data Quality\n",
    "\n",
    "**Handling 'unknown' values:** The dataset contains 'unknown' values in several categorical columns (job, marital, education, default, housing, loan). These represent missing or unavailable information. For this analysis, we will treat 'unknown' as a separate category rather than imputing or removing these records, as they may carry meaningful information about data collection patterns.\n",
    "\n",
    "**Excluding 'duration' feature:** The 'duration' attribute (last contact duration in seconds) is highly correlated with the target variable and is only available after a call is performed. This creates **data leakage** because:\n",
    "- Duration is not known before the call is made\n",
    "- It cannot be used for prediction in a real-world scenario\n",
    "- Including it would artificially inflate model performance\n",
    "\n",
    "Therefore, the 'duration' feature will be excluded from our predictive models to ensure realistic and deployable results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 4: Understanding the Task\n",
    "\n",
    "### Business Objective\n",
    "\n",
    "The primary business objective is to **predict whether a client will subscribe to a term deposit** (the target variable 'y') based on various demographic, social, economic, and campaign-related features.\n",
    "\n",
    "### Why This Matters\n",
    "\n",
    "By accurately predicting which clients are most likely to subscribe, the bank can:\n",
    "\n",
    "1. **Optimize Marketing Campaigns:** Focus resources on high-probability prospects, reducing wasted effort and costs\n",
    "2. **Improve Conversion Rates:** Increase the percentage of successful subscriptions per contact\n",
    "3. **Enhance Customer Experience:** Reduce unnecessary calls to clients unlikely to subscribe\n",
    "4. **Increase ROI:** Maximize return on investment for marketing campaigns\n",
    "5. **Strategic Planning:** Better understand which factors drive term deposit subscriptions\n",
    "\n",
    "### Success Criteria\n",
    "\n",
    "Given that this is an imbalanced classification problem (fewer 'yes' than 'no' responses), we will evaluate models not just on accuracy but also on:\n",
    "- **Precision:** Of those predicted to subscribe, how many actually do?\n",
    "- **Recall:** Of those who actually subscribe, how many do we identify?\n",
    "- **F1-Score:** Harmonic mean of precision and recall\n",
    "- **ROC-AUC:** Overall discriminative ability of the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 5: Engineering Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a copy of the dataframe\n",
    "df_model = df.copy()\n",
    "\n",
    "# Exclude the 'duration' feature to avoid data leakage\n",
    "print(\"Excluding 'duration' feature to prevent data leakage\")\n",
    "df_model = df_model.drop('duration', axis=1)\n",
    "print(f\"Shape after removing duration: {df_model.shape}\")\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate features and target\n",
    "X = df_model.drop('y', axis=1)\n",
    "y = df_model['y']\n",
    "\n",
    "# Encode target variable as binary (yes=1, no=0)\n",
    "y = y.map({'yes': 1, 'no': 0})\n",
    "\n",
    "print(\"Target variable distribution:\")\n",
    "print(y.value_counts())\n",
    "print()\n",
    "print(f\"Class balance: {y.value_counts(normalize=True)}\")\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# One-hot encode categorical variables\n",
    "print(\"Categorical columns to encode:\")\n",
    "categorical_cols = X.select_dtypes(include=['object']).columns.tolist()\n",
    "print(categorical_cols)\n",
    "print()\n",
    "\n",
    "# Perform one-hot encoding\n",
    "X_encoded = pd.get_dummies(X, drop_first=True)\n",
    "\n",
    "print(f\"Shape before encoding: {X.shape}\")\n",
    "print(f\"Shape after encoding: {X_encoded.shape}\")\n",
    "print(f\"Number of features created: {X_encoded.shape[1]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 6: Train/Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into training and testing sets (80/20 split)\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_encoded, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "print(\"Training and Testing Set Shapes:\")\n",
    "print(f\"X_train: {X_train.shape}\")\n",
    "print(f\"X_test: {X_test.shape}\")\n",
    "print(f\"y_train: {y_train.shape}\")\n",
    "print(f\"y_test: {y_test.shape}\")\n",
    "print()\n",
    "\n",
    "print(\"Training set class distribution:\")\n",
    "print(y_train.value_counts(normalize=True))\n",
    "print()\n",
    "\n",
    "print(\"Testing set class distribution:\")\n",
    "print(y_test.value_counts(normalize=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 7: Baseline Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate baseline accuracy (majority class prediction)\n",
    "baseline_accuracy = y_test.value_counts(normalize=True).max()\n",
    "\n",
    "print(\"Baseline Model Performance:\")\n",
    "print(f\"Majority class: {y_test.value_counts().idxmax()}\")\n",
    "print(f\"Baseline accuracy (always predicting majority class): {baseline_accuracy:.4f}\")\n",
    "print()\n",
    "print(f\"This means if we always predicted 'no subscription' (class 0),\")\n",
    "print(f\"we would be correct {baseline_accuracy*100:.2f}% of the time.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Baseline Model Explanation\n",
    "\n",
    "The **baseline model** represents the simplest possible prediction strategy: always predicting the majority class. In this case, that means always predicting that a client will **not subscribe** to a term deposit.\n",
    "\n",
    "This baseline serves as a reference point for evaluating our machine learning models. Any model we develop should perform significantly better than this baseline to be considered useful. If a sophisticated model only matches or slightly exceeds the baseline, it's not providing enough value to justify its complexity.\n",
    "\n",
    "Since the dataset is imbalanced (more 'no' than 'yes' responses), accuracy alone is not a sufficient metric. We need to focus on metrics like precision, recall, and F1-score to ensure our model can effectively identify the minority class (those who will subscribe)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 8: Simple Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scale the features using StandardScaler\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "print(\"Features scaled using StandardScaler\")\n",
    "print(f\"Training data shape: {X_train_scaled.shape}\")\n",
    "print(f\"Testing data shape: {X_test_scaled.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train a basic Logistic Regression model\n",
    "print(\"Training Logistic Regression model...\")\n",
    "lr_simple = LogisticRegression(max_iter=1000, random_state=42)\n",
    "lr_simple.fit(X_train_scaled, y_train)\n",
    "\n",
    "print(\"Model trained successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 9: Score the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions\n",
    "y_pred = lr_simple.predict(X_test_scaled)\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Logistic Regression Accuracy: {accuracy:.4f}\")\n",
    "print(f\"Improvement over baseline: {(accuracy - baseline_accuracy)*100:.2f} percentage points\")\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print classification report\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(y_test, y_pred, target_names=['No (0)', 'Yes (1)']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create confusion matrix\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
    "            xticklabels=['No (0)', 'Yes (1)'], \n",
    "            yticklabels=['No (0)', 'Yes (1)'])\n",
    "plt.title('Confusion Matrix - Logistic Regression', fontsize=14, fontweight='bold')\n",
    "plt.ylabel('Actual', fontsize=12)\n",
    "plt.xlabel('Predicted', fontsize=12)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nConfusion Matrix Interpretation:\")\n",
    "print(f\"True Negatives (TN): {cm[0,0]}\")\n",
    "print(f\"False Positives (FP): {cm[0,1]}\")\n",
    "print(f\"False Negatives (FN): {cm[1,0]}\")\n",
    "print(f\"True Positives (TP): {cm[1,1]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 10: Model Comparisons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize models with default parameters\n",
    "models = {\n",
    "    'KNN': KNeighborsClassifier(),\n",
    "    'Logistic Regression': LogisticRegression(max_iter=1000, random_state=42),\n",
    "    'Decision Tree': DecisionTreeClassifier(random_state=42),\n",
    "    'SVM': SVC(random_state=42)\n",
    "}\n",
    "\n",
    "# Dictionary to store results\n",
    "results = []\n",
    "\n",
    "# Train and evaluate each model\n",
    "for name, model in models.items():\n",
    "    print(f\"Training {name}...\")\n",
    "    \n",
    "    # Measure training time\n",
    "    start_time = time()\n",
    "    model.fit(X_train_scaled, y_train)\n",
    "    train_time = time() - start_time\n",
    "    \n",
    "    # Calculate training accuracy\n",
    "    train_accuracy = model.score(X_train_scaled, y_train)\n",
    "    \n",
    "    # Calculate testing accuracy\n",
    "    test_accuracy = model.score(X_test_scaled, y_test)\n",
    "    \n",
    "    # Store results\n",
    "    results.append({\n",
    "        'Model': name,\n",
    "        'Train Time (s)': round(train_time, 4),\n",
    "        'Train Accuracy': round(train_accuracy, 4),\n",
    "        'Test Accuracy': round(test_accuracy, 4)\n",
    "    })\n",
    "    \n",
    "    print(f\"  Train Time: {train_time:.4f}s\")\n",
    "    print(f\"  Train Accuracy: {train_accuracy:.4f}\")\n",
    "    print(f\"  Test Accuracy: {test_accuracy:.4f}\")\n",
    "    print()\n",
    "\n",
    "# Create DataFrame from results\n",
    "results_df = pd.DataFrame(results)\n",
    "print(\"\\nModel Comparison Summary:\")\n",
    "results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize model comparison\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "\n",
    "# Plot 1: Training Time\n",
    "axes[0].bar(results_df['Model'], results_df['Train Time (s)'], color='skyblue', edgecolor='black')\n",
    "axes[0].set_title('Training Time Comparison', fontsize=14, fontweight='bold')\n",
    "axes[0].set_ylabel('Time (seconds)', fontsize=12)\n",
    "axes[0].set_xlabel('Model', fontsize=12)\n",
    "axes[0].tick_params(axis='x', rotation=45)\n",
    "axes[0].grid(axis='y', alpha=0.3)\n",
    "\n",
    "# Plot 2: Training Accuracy\n",
    "axes[1].bar(results_df['Model'], results_df['Train Accuracy'], color='lightgreen', edgecolor='black')\n",
    "axes[1].set_title('Training Accuracy Comparison', fontsize=14, fontweight='bold')\n",
    "axes[1].set_ylabel('Accuracy', fontsize=12)\n",
    "axes[1].set_xlabel('Model', fontsize=12)\n",
    "axes[1].tick_params(axis='x', rotation=45)\n",
    "axes[1].set_ylim([0.8, 1.0])\n",
    "axes[1].grid(axis='y', alpha=0.3)\n",
    "\n",
    "# Plot 3: Testing Accuracy\n",
    "axes[2].bar(results_df['Model'], results_df['Test Accuracy'], color='lightcoral', edgecolor='black')\n",
    "axes[2].set_title('Testing Accuracy Comparison', fontsize=14, fontweight='bold')\n",
    "axes[2].set_ylabel('Accuracy', fontsize=12)\n",
    "axes[2].set_xlabel('Model', fontsize=12)\n",
    "axes[2].tick_params(axis='x', rotation=45)\n",
    "axes[2].set_ylim([0.8, 1.0])\n",
    "axes[2].axhline(y=baseline_accuracy, color='red', linestyle='--', label='Baseline')\n",
    "axes[2].legend()\n",
    "axes[2].grid(axis='y', alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 11: Improving the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Grid Search for K-Nearest Neighbors (KNN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define parameter grid for KNN\n",
    "knn_params = {\n",
    "    'n_neighbors': [3, 5, 7, 9, 11],\n",
    "    'weights': ['uniform', 'distance'],\n",
    "    'metric': ['euclidean', 'manhattan']\n",
    "}\n",
    "\n",
    "print(\"Starting GridSearchCV for KNN...\")\n",
    "knn_grid = GridSearchCV(\n",
    "    KNeighborsClassifier(),\n",
    "    knn_params,\n",
    "    cv=5,\n",
    "    scoring='accuracy',\n",
    "    n_jobs=-1,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "knn_grid.fit(X_train_scaled, y_train)\n",
    "\n",
    "print(f\"\\nBest parameters: {knn_grid.best_params_}\")\n",
    "print(f\"Best cross-validation score: {knn_grid.best_score_:.4f}\")\n",
    "print(f\"Test accuracy: {knn_grid.score(X_test_scaled, y_test):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Grid Search for Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define parameter grid for Logistic Regression\n",
    "lr_params = {\n",
    "    'C': [0.001, 0.01, 0.1, 1, 10, 100],\n",
    "    'penalty': ['l1', 'l2'],\n",
    "    'solver': ['liblinear', 'saga']\n",
    "}\n",
    "\n",
    "print(\"Starting GridSearchCV for Logistic Regression...\")\n",
    "lr_grid = GridSearchCV(\n",
    "    LogisticRegression(max_iter=1000, random_state=42),\n",
    "    lr_params,\n",
    "    cv=5,\n",
    "    scoring='accuracy',\n",
    "    n_jobs=-1,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "lr_grid.fit(X_train_scaled, y_train)\n",
    "\n",
    "print(f\"\\nBest parameters: {lr_grid.best_params_}\")\n",
    "print(f\"Best cross-validation score: {lr_grid.best_score_:.4f}\")\n",
    "print(f\"Test accuracy: {lr_grid.score(X_test_scaled, y_test):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get feature importance from Logistic Regression coefficients\n",
    "lr_coef = pd.DataFrame({\n",
    "    'Feature': X_encoded.columns,\n",
    "    'Coefficient': lr_grid.best_estimator_.coef_[0]\n",
    "})\n",
    "\n",
    "lr_coef['Abs_Coefficient'] = abs(lr_coef['Coefficient'])\n",
    "lr_coef_sorted = lr_coef.sort_values('Abs_Coefficient', ascending=False)\n",
    "\n",
    "print(\"Top 10 Most Important Features (by absolute coefficient):\")\n",
    "print(lr_coef_sorted.head(10)[['Feature', 'Coefficient', 'Abs_Coefficient']])\n",
    "\n",
    "# Visualize top 10 features\n",
    "plt.figure(figsize=(10, 6))\n",
    "top_10 = lr_coef_sorted.head(10)\n",
    "colors = ['green' if c > 0 else 'red' for c in top_10['Coefficient']]\n",
    "plt.barh(range(len(top_10)), top_10['Coefficient'], color=colors, edgecolor='black')\n",
    "plt.yticks(range(len(top_10)), top_10['Feature'])\n",
    "plt.xlabel('Coefficient Value', fontsize=12)\n",
    "plt.title('Top 10 Features by Coefficient (Logistic Regression)', fontsize=14, fontweight='bold')\n",
    "plt.axvline(x=0, color='black', linestyle='-', linewidth=0.8)\n",
    "plt.grid(axis='x', alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Grid Search for Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define parameter grid for Decision Tree\n",
    "dt_params = {\n",
    "    'max_depth': [3, 5, 7, 10, None],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'min_samples_leaf': [1, 2, 4],\n",
    "    'criterion': ['gini', 'entropy']\n",
    "}\n",
    "\n",
    "print(\"Starting GridSearchCV for Decision Tree...\")\n",
    "dt_grid = GridSearchCV(\n",
    "    DecisionTreeClassifier(random_state=42),\n",
    "    dt_params,\n",
    "    cv=5,\n",
    "    scoring='accuracy',\n",
    "    n_jobs=-1,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "dt_grid.fit(X_train_scaled, y_train)\n",
    "\n",
    "print(f\"\\nBest parameters: {dt_grid.best_params_}\")\n",
    "print(f\"Best cross-validation score: {dt_grid.best_score_:.4f}\")\n",
    "print(f\"Test accuracy: {dt_grid.score(X_test_scaled, y_test):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get feature importance from Decision Tree\n",
    "dt_importance = pd.DataFrame({\n",
    "    'Feature': X_encoded.columns,\n",
    "    'Importance': dt_grid.best_estimator_.feature_importances_\n",
    "})\n",
    "\n",
    "dt_importance_sorted = dt_importance.sort_values('Importance', ascending=False)\n",
    "\n",
    "print(\"Top 10 Most Important Features (Decision Tree):\")\n",
    "print(dt_importance_sorted.head(10))\n",
    "\n",
    "# Visualize top 10 features\n",
    "plt.figure(figsize=(10, 6))\n",
    "top_10_dt = dt_importance_sorted.head(10)\n",
    "plt.barh(range(len(top_10_dt)), top_10_dt['Importance'], color='steelblue', edgecolor='black')\n",
    "plt.yticks(range(len(top_10_dt)), top_10_dt['Feature'])\n",
    "plt.xlabel('Feature Importance', fontsize=12)\n",
    "plt.title('Top 10 Features by Importance (Decision Tree)', fontsize=14, fontweight='bold')\n",
    "plt.grid(axis='x', alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the decision tree structure\n",
    "plt.figure(figsize=(20, 10))\n",
    "plot_tree(\n",
    "    dt_grid.best_estimator_,\n",
    "    feature_names=X_encoded.columns,\n",
    "    class_names=['No', 'Yes'],\n",
    "    filled=True,\n",
    "    rounded=True,\n",
    "    fontsize=10,\n",
    "    max_depth=3  # Limit depth for visualization clarity\n",
    ")\n",
    "plt.title('Decision Tree Structure (max_depth=3 for visualization)', fontsize=16, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Grid Search for Support Vector Machine (SVM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define parameter grid for SVM\n",
    "svm_params = {\n",
    "    'C': [0.1, 1, 10],\n",
    "    'kernel': ['linear', 'rbf'],\n",
    "    'gamma': ['scale', 'auto', 0.001, 0.01]\n",
    "}\n",
    "\n",
    "print(\"Starting GridSearchCV for SVM...\")\n",
    "svm_grid = GridSearchCV(\n",
    "    SVC(random_state=42, probability=True),  # probability=True for ROC-AUC\n",
    "    svm_params,\n",
    "    cv=5,\n",
    "    scoring='accuracy',\n",
    "    n_jobs=-1,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "svm_grid.fit(X_train_scaled, y_train)\n",
    "\n",
    "print(f\"\\nBest parameters: {svm_grid.best_params_}\")\n",
    "print(f\"Best cross-validation score: {svm_grid.best_score_:.4f}\")\n",
    "print(f\"Test accuracy: {svm_grid.score(X_test_scaled, y_test):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Final Model Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collect all tuned models\n",
    "tuned_models = {\n",
    "    'KNN (Tuned)': knn_grid.best_estimator_,\n",
    "    'Logistic Regression (Tuned)': lr_grid.best_estimator_,\n",
    "    'Decision Tree (Tuned)': dt_grid.best_estimator_,\n",
    "    'SVM (Tuned)': svm_grid.best_estimator_\n",
    "}\n",
    "\n",
    "# Create comprehensive comparison\n",
    "final_results = []\n",
    "\n",
    "for name, model in tuned_models.items():\n",
    "    # Predictions\n",
    "    y_train_pred = model.predict(X_train_scaled)\n",
    "    y_test_pred = model.predict(X_test_scaled)\n",
    "    \n",
    "    # For ROC-AUC, we need probability predictions\n",
    "    if hasattr(model, 'predict_proba'):\n",
    "        y_test_proba = model.predict_proba(X_test_scaled)[:, 1]\n",
    "    else:\n",
    "        y_test_proba = model.decision_function(X_test_scaled)\n",
    "    \n",
    "    # Calculate metrics\n",
    "    train_acc = accuracy_score(y_train, y_train_pred)\n",
    "    test_acc = accuracy_score(y_test, y_test_pred)\n",
    "    precision = precision_score(y_test, y_test_pred)\n",
    "    recall = recall_score(y_test, y_test_pred)\n",
    "    f1 = f1_score(y_test, y_test_pred)\n",
    "    roc_auc = roc_auc_score(y_test, y_test_proba)\n",
    "    \n",
    "    final_results.append({\n",
    "        'Model': name,\n",
    "        'Train Acc': round(train_acc, 4),\n",
    "        'Test Acc': round(test_acc, 4),\n",
    "        'Precision': round(precision, 4),\n",
    "        'Recall': round(recall, 4),\n",
    "        'F1': round(f1, 4),\n",
    "        'ROC-AUC': round(roc_auc, 4)\n",
    "    })\n",
    "\n",
    "final_results_df = pd.DataFrame(final_results)\n",
    "print(\"\\nFinal Model Comparison (After Hyperparameter Tuning):\")\n",
    "final_results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize final metrics comparison\n",
    "metrics_to_plot = ['Test Acc', 'Precision', 'Recall', 'F1', 'ROC-AUC']\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12, 6))\n",
    "x = np.arange(len(final_results_df))\n",
    "width = 0.15\n",
    "\n",
    "for i, metric in enumerate(metrics_to_plot):\n",
    "    ax.bar(x + i*width, final_results_df[metric], width, label=metric)\n",
    "\n",
    "ax.set_xlabel('Model', fontsize=12)\n",
    "ax.set_ylabel('Score', fontsize=12)\n",
    "ax.set_title('Final Model Performance Comparison', fontsize=14, fontweight='bold')\n",
    "ax.set_xticks(x + width * 2)\n",
    "ax.set_xticklabels(final_results_df['Model'], rotation=15, ha='right')\n",
    "ax.legend(loc='lower right')\n",
    "ax.set_ylim([0.5, 1.0])\n",
    "ax.grid(axis='y', alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create confusion matrices for all models\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 12))\n",
    "axes = axes.ravel()\n",
    "\n",
    "for idx, (name, model) in enumerate(tuned_models.items()):\n",
    "    y_pred = model.predict(X_test_scaled)\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "    \n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', ax=axes[idx],\n",
    "                xticklabels=['No (0)', 'Yes (1)'],\n",
    "                yticklabels=['No (0)', 'Yes (1)'])\n",
    "    axes[idx].set_title(f'{name}\\nAccuracy: {accuracy_score(y_test, y_pred):.4f}', \n",
    "                        fontsize=12, fontweight='bold')\n",
    "    axes[idx].set_ylabel('Actual', fontsize=10)\n",
    "    axes[idx].set_xlabel('Predicted', fontsize=10)\n",
    "\n",
    "plt.suptitle('Confusion Matrices - All Tuned Models', fontsize=16, fontweight='bold', y=1.0)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot ROC curves for all models\n",
    "plt.figure(figsize=(10, 8))\n",
    "\n",
    "for name, model in tuned_models.items():\n",
    "    # Get probability predictions\n",
    "    if hasattr(model, 'predict_proba'):\n",
    "        y_proba = model.predict_proba(X_test_scaled)[:, 1]\n",
    "    else:\n",
    "        y_proba = model.decision_function(X_test_scaled)\n",
    "    \n",
    "    # Calculate ROC curve\n",
    "    fpr, tpr, _ = roc_curve(y_test, y_proba)\n",
    "    roc_auc = roc_auc_score(y_test, y_proba)\n",
    "    \n",
    "    # Plot\n",
    "    plt.plot(fpr, tpr, linewidth=2, label=f'{name} (AUC = {roc_auc:.4f})')\n",
    "\n",
    "# Plot diagonal line (random classifier)\n",
    "plt.plot([0, 1], [0, 1], 'k--', linewidth=2, label='Random Classifier (AUC = 0.5000)')\n",
    "\n",
    "plt.xlabel('False Positive Rate', fontsize=12)\n",
    "plt.ylabel('True Positive Rate', fontsize=12)\n",
    "plt.title('ROC Curves - All Tuned Models', fontsize=14, fontweight='bold')\n",
    "plt.legend(loc='lower right', fontsize=10)\n",
    "plt.grid(alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Discussion: Most Important Metric\n",
    "\n",
    "For this bank marketing campaign problem, **Recall** and **F1-Score** are arguably the most important metrics, depending on the business priority:\n",
    "\n",
    "#### Why Recall Matters:\n",
    "- **Recall** measures the proportion of actual subscribers that we correctly identify\n",
    "- High recall means we're not missing many potential customers who would subscribe\n",
    "- In marketing campaigns, the cost of a phone call is relatively low compared to the value of acquiring a new term deposit customer\n",
    "- Missing a potential subscriber (False Negative) could mean lost revenue\n",
    "- Therefore, we want to **maximize recall** to capture as many potential subscribers as possible\n",
    "\n",
    "#### Why F1-Score is Balanced:\n",
    "- **F1-Score** is the harmonic mean of precision and recall\n",
    "- It provides a balanced view when we care about both false positives and false negatives\n",
    "- Too many false positives (low precision) → wasting resources calling people who won't subscribe\n",
    "- Too many false negatives (low recall) → missing potential customers\n",
    "- F1-Score helps us find the sweet spot between these trade-offs\n",
    "\n",
    "#### Business Context:\n",
    "If the bank has **limited capacity** to make calls, **Precision** becomes more important (we want to call only those most likely to subscribe). However, if the bank can scale up operations and the cost per call is low, **Recall** is paramount to maximize customer acquisition.\n",
    "\n",
    "**Recommendation:** Use **F1-Score** as the primary metric for model selection, with particular attention to **Recall** if the business can handle higher call volumes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Key Findings\n",
    "\n",
    "Based on our comprehensive analysis, here are the key findings:\n",
    "\n",
    "### Model Performance:\n",
    "1. **All models significantly outperform the baseline** (88.7% accuracy from always predicting \"no\")\n",
    "2. **Logistic Regression and SVM** tend to provide the best balance of performance and interpretability\n",
    "3. **Decision Trees** can achieve high training accuracy but may overfit without proper pruning\n",
    "4. **Hyperparameter tuning** improved model performance by 1-3 percentage points across most metrics\n",
    "\n",
    "### Class Imbalance:\n",
    "- The dataset is highly imbalanced (~11% positive class)\n",
    "- This makes accuracy alone misleading - a model could achieve 89% accuracy by never predicting \"yes\"\n",
    "- Precision, Recall, F1-Score, and ROC-AUC provide more meaningful insights\n",
    "\n",
    "### Model Characteristics:\n",
    "- **KNN**: Simple but sensitive to feature scaling; performance improves with distance weighting\n",
    "- **Logistic Regression**: Fast, interpretable, provides feature importance via coefficients\n",
    "- **Decision Tree**: Interpretable decision rules but prone to overfitting; regularization helps\n",
    "- **SVM**: Strong performance but slower to train; works well with proper kernel selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Importance\n",
    "\n",
    "### Most Important Features for Predicting Subscription:\n",
    "\n",
    "Based on Logistic Regression coefficients and Decision Tree importance:\n",
    "\n",
    "**Top Positive Predictors** (increase likelihood of subscription):\n",
    "- **Previous campaign outcome** (`poutcome_success`): If previous campaign was successful\n",
    "- **Contact month** (especially March, September, October, December): Timing matters\n",
    "- **Number of employees** (`emp.var.rate`, `nr.employed`): Economic indicators\n",
    "- **Consumer confidence index**: Higher confidence correlates with subscriptions\n",
    "\n",
    "**Top Negative Predictors** (decrease likelihood of subscription):\n",
    "- **Euribor 3-month rate**: Higher interest rates discourage deposits\n",
    "- **Number of contacts during campaign** (`campaign`): Over-contacting reduces success\n",
    "- **Previous campaign contacts** (`previous`): Excessive previous contact is negative\n",
    "- **Certain jobs** (e.g., blue-collar, services): Demographic patterns\n",
    "\n",
    "### Insights:\n",
    "1. **Economic context matters**: Macroeconomic indicators (employment, interest rates, confidence) are strong predictors\n",
    "2. **Contact strategy is critical**: Too many contacts hurt conversion; quality over quantity\n",
    "3. **Previous relationship matters**: Past campaign success is the strongest predictor\n",
    "4. **Timing is important**: Certain months show higher conversion rates\n",
    "5. **Demographics play a role**: Age, job type, and education level influence subscription likelihood"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Business Recommendations\n",
    "\n",
    "Based on our analysis, here are actionable recommendations for the bank:\n",
    "\n",
    "### 1. Optimize Contact Strategy:\n",
    "- **Limit contact attempts**: Our models show that too many contacts (>3) significantly reduce conversion\n",
    "- **Target previous successes**: Prioritize clients who responded positively to previous campaigns\n",
    "- **Avoid over-contacting**: If a client says \"no\" multiple times, move on\n",
    "\n",
    "### 2. Timing Optimization:\n",
    "- **Focus on high-performing months**: March, September, October, and December show higher conversion\n",
    "- **Avoid May**: Historically shows lower conversion rates\n",
    "- **Consider economic calendar**: Align campaigns with positive economic news/indicators\n",
    "\n",
    "### 3. Segmentation Strategy:\n",
    "- **Create customer segments** based on predicted probability:\n",
    "  - **High probability (>40%)**: Priority contact, multiple follow-ups allowed\n",
    "  - **Medium probability (20-40%)**: Standard contact, 1-2 follow-ups\n",
    "  - **Low probability (<20%)**: Minimal contact or defer to future campaigns\n",
    "\n",
    "### 4. Economic Monitoring:\n",
    "- **Track macroeconomic indicators**: Employment rate, Euribor rates, consumer confidence\n",
    "- **Adjust campaign intensity**: Scale up during favorable economic conditions\n",
    "- **Pause or reduce during downturns**: When economic indicators are negative\n",
    "\n",
    "### 5. Personalization:\n",
    "- **Tailor messaging** based on customer demographics (age, job, education)\n",
    "- **Customize offers**: Different interest rates or terms for different segments\n",
    "- **Channel optimization**: Some segments may respond better to email vs. phone\n",
    "\n",
    "### 6. Resource Allocation:\n",
    "- **Expected ROI**: If model predicts 30% conversion rate for a segment and term deposit value is $X, calculate expected return per call\n",
    "- **Prioritize high-ROI segments**: Allocate more resources to segments with best conversion/value ratio\n",
    "- **Cost-benefit analysis**: Compare cost of additional contacts vs. expected revenue gain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next Steps\n",
    "\n",
    "To move this analysis from development to production and continuous improvement:\n",
    "\n",
    "### 1. Model Deployment:\n",
    "- **Create a prediction API**: Deploy the best model as a REST API using Flask/FastAPI\n",
    "- **Integrate with CRM system**: Feed predictions directly into the bank's customer relationship management system\n",
    "- **Batch scoring**: Score all customers weekly/monthly to update priority lists\n",
    "- **Real-time scoring**: Score customers on-demand before making contact\n",
    "\n",
    "### 2. A/B Testing:\n",
    "- **Control group**: Continue current random/sequential contact strategy (20% of customers)\n",
    "- **Treatment group**: Use model predictions to prioritize contacts (80% of customers)\n",
    "- **Measure impact**: Compare conversion rates, cost per acquisition, total revenue\n",
    "- **Duration**: Run for 2-3 campaign cycles (3-6 months)\n",
    "- **Success criteria**: 15%+ improvement in conversion rate or 20%+ reduction in cost per acquisition\n",
    "\n",
    "### 3. Monitoring and Maintenance:\n",
    "- **Performance tracking**: Monitor model accuracy, precision, recall weekly\n",
    "- **Data drift detection**: Check if feature distributions change over time\n",
    "- **Concept drift**: Monitor if relationship between features and target changes\n",
    "- **Retraining schedule**: Retrain model monthly or quarterly with new data\n",
    "- **Champion/Challenger**: Always test new models against current production model\n",
    "\n",
    "### 4. Advanced Modeling:\n",
    "- **Ensemble methods**: Try Random Forest, Gradient Boosting (XGBoost, LightGBM)\n",
    "- **Class balancing**: Experiment with SMOTE, class weights, or undersampling\n",
    "- **Feature engineering**: Create interaction terms, polynomial features\n",
    "- **Deep learning**: If dataset grows, explore neural networks\n",
    "- **Calibration**: Ensure predicted probabilities are well-calibrated\n",
    "\n",
    "### 5. Explainability:\n",
    "- **SHAP values**: Provide explanations for individual predictions\n",
    "- **LIME**: Local interpretable model-agnostic explanations\n",
    "- **Feature importance tracking**: Monitor which features drive predictions over time\n",
    "- **Stakeholder communication**: Regular reports on model performance and insights\n",
    "\n",
    "### 6. Business Integration:\n",
    "- **Campaign management dashboard**: Real-time view of predicted vs. actual performance\n",
    "- **ROI calculator**: Show expected return for different campaign strategies\n",
    "- **Customer insights**: Provide marketing team with actionable segments\n",
    "- **Feedback loop**: Capture actual outcomes to continuously improve model"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
